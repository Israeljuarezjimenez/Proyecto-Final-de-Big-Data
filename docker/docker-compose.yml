services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    environment:
      - CLUSTER_NAME=tlc
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9870:9870"
    volumes:
      - ./hadoop_namenode:/hadoop/dfs/name
      - ../:/opt/project

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9864:9864"
    volumes:
      - ./hadoop_datanode:/hadoop/dfs/data
    depends_on:
      - namenode

  spark-master:
    build:
      context: ..
      dockerfile: docker/Dockerfile.spark
      args:
        BASE_IMAGE: bde2020/spark-master:3.2.1-hadoop3.2
    image: tlc-spark-master:3.2.1
    container_name: spark-master
    hostname: spark-master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - PYTHONPATH=/opt/project:/spark/python:/spark/python/lib/py4j-0.10.9.3-src.zip
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - PATH=/spark/bin:$PATH
    ports:
      - "18080:8080"
      - "7077:7077"
    volumes:
      - ../:/opt/project
    depends_on:
      - namenode

  spark-worker:
    build:
      context: ..
      dockerfile: docker/Dockerfile.spark
      args:
        BASE_IMAGE: bde2020/spark-worker:3.2.1-hadoop3.2
    image: tlc-spark-worker:3.2.1
    container_name: spark-worker
    hostname: spark-worker
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYTHONPATH=/opt/project:/spark/python:/spark/python/lib/py4j-0.10.9.3-src.zip
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - PATH=/spark/bin:$PATH
    ports:
      - "18081:8081"
    volumes:
      - ../:/opt/project
    depends_on:
      - spark-master
